{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiNxdujPCJ0b",
        "outputId": "9e013434-4059-44d3-8bcc-9f50ffc23162",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.3)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install networkx tqdm\n",
        "!pip install openai tiktoken --quiet\n",
        "\n",
        "!pip -q install pyvis\n",
        "!pip -q install -U jinja2 markupsafe pyvis\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_PATH = \"/content/drive/MyDrive/ProjectsforFinalproject/flaskbb\"\n"
      ],
      "metadata": {
        "id": "EIijbSA34bm8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# پیدا کردن فایل های پایتون و ایگنور فایل های غیر ضروری\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "def find_python_files(path):\n",
        "    py_files = []\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        if any(x in root for x in [\"venv\", \"env\", \"__pycache__\", \"migrations\", \"tests\"]):\n",
        "            continue\n",
        "\n",
        "        for f in files:\n",
        "            if f.endswith(\".py\"):\n",
        "                py_files.append(os.path.join(root, f))\n",
        "    return py_files\n",
        "\n",
        "python_files = find_python_files(PROJECT_PATH)\n",
        "len(python_files), python_files[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f61uO8xxCz6K",
        "outputId": "1f702574-b1bc-4bb8-98ff-376bcfbf857c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90,\n",
              " ['/content/drive/MyDrive/ProjectsforFinalproject/flaskbb/celery_worker.py',\n",
              "  '/content/drive/MyDrive/ProjectsforFinalproject/flaskbb/wsgi.py',\n",
              "  '/content/drive/MyDrive/ProjectsforFinalproject/flaskbb/setup.py',\n",
              "  '/content/drive/MyDrive/ProjectsforFinalproject/flaskbb/docs/conf.py',\n",
              "  '/content/drive/MyDrive/ProjectsforFinalproject/flaskbb/flaskbb/extensions.py'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  استخراح و ساخت AST\n",
        "import ast\n",
        "import json\n",
        "\n",
        "nodes = []\n",
        "edges = []\n",
        "\n",
        "\n",
        "node_id_counter = 0\n",
        "\n",
        "# def new_node(n_type, name, file_path, extra=None):\n",
        "def new_node(n_type, name, file_path):\n",
        "    \"\"\"ایجاد یک نود جدید\"\"\"\n",
        "    global node_id_counter\n",
        "    nid = node_id_counter\n",
        "    node_id_counter += 1\n",
        "\n",
        "    nodes.append({\n",
        "        \"id\": nid,\n",
        "        \"type\": n_type,\n",
        "        \"name\": name,\n",
        "        \"file\": file_path,\n",
        "        # \"extra\": extra or {}\n",
        "    })\n",
        "    return nid\n",
        "\n",
        "\n",
        "# نودهای فایل\n",
        "file_nodes = {}   # map: file_path → node_id\n",
        "for f in python_files:\n",
        "    file_nodes[f] = new_node(\"file\", os.path.basename(f), f)\n",
        "\n",
        "# استخراج Class و Function\n",
        "\n",
        "function_nodes = {}   # map: (file, func_name) → node_id\n",
        "class_nodes = {}      # map: (file, class_name) → node_id\n",
        "\n",
        "for f in tqdm(python_files):\n",
        "    try:\n",
        "        src = open(f, \"r\", encoding=\"utf-8\").read()\n",
        "        tree = ast.parse(src)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    for node in ast.walk(tree):\n",
        "        if isinstance(node, ast.ClassDef):\n",
        "            cid = new_node(\"class\", node.name, f)\n",
        "            class_nodes[(f, node.name)] = cid\n",
        "\n",
        "            # یال File → Class\n",
        "            edges.append({\n",
        "                \"src\": file_nodes[f],\n",
        "                \"dst\": cid,\n",
        "                \"type\": \"contains\"\n",
        "            })\n",
        "\n",
        "        elif isinstance(node, ast.FunctionDef):\n",
        "            fid = new_node(\"function\", node.name, f)\n",
        "            function_nodes[(f, node.name)] = fid\n",
        "\n",
        "            # یال File → Function\n",
        "            edges.append({\n",
        "                \"src\": file_nodes[f],\n",
        "                \"dst\": fid,\n",
        "                \"type\": \"contains\"\n",
        "            })"
      ],
      "metadata": {
        "id": "WJxyhs6tDeW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ba284a-373a-4726-c146-f69db7d67f89"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [00:37<00:00,  2.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# #Filling Extra node value commented for now\n",
        "# import os\n",
        "# import ast\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# nodes = []\n",
        "# edges = []\n",
        "# node_id_counter = 0\n",
        "\n",
        "# def new_node(n_type, name, file_path, extra=None):\n",
        "#     \"\"\"ایجاد یک نود جدید\"\"\"\n",
        "#     global node_id_counter\n",
        "#     nid = node_id_counter\n",
        "#     node_id_counter += 1\n",
        "\n",
        "#     nodes.append({\n",
        "#         \"id\": nid,\n",
        "#         \"type\": n_type,\n",
        "#         \"name\": name,\n",
        "#         \"file\": file_path,\n",
        "#         \"extra\": extra or {}\n",
        "#     })\n",
        "#     return nid\n",
        "\n",
        "\n",
        "# # -----------------------------\n",
        "# # 1) ساخت نودهای فایل\n",
        "# # -----------------------------\n",
        "# file_nodes = {}  # file_path -> node_id\n",
        "# for f in python_files:\n",
        "#     file_nodes[f] = new_node(\n",
        "#         \"file\",\n",
        "#         os.path.basename(f),\n",
        "#         f,\n",
        "#         extra={\"file_path\": f}\n",
        "#     )\n",
        "\n",
        "\n",
        "# # -----------------------------\n",
        "# # 2) Visitor برای استخراج کلاس/تابع با context\n",
        "# # -----------------------------\n",
        "# class CodeEntityVisitor(ast.NodeVisitor):\n",
        "#     def __init__(self, file_path):\n",
        "#         self.file_path = file_path\n",
        "#         self.class_stack = []   # برای تشخیص اینکه داخل کلاس هستیم یا نه\n",
        "\n",
        "#     def _get_lines(self, node):\n",
        "#         start = getattr(node, \"lineno\", None)\n",
        "#         end = getattr(node, \"end_lineno\", None)\n",
        "#         return start, end\n",
        "\n",
        "#     def _get_docstring(self, node):\n",
        "#         try:\n",
        "#             return ast.get_docstring(node)\n",
        "#         except Exception:\n",
        "#             return None\n",
        "\n",
        "#     def _get_decorators(self, node):\n",
        "#         # decorator ها را به شکل متن ساده نگه می‌داریم\n",
        "#         decs = []\n",
        "#         for d in getattr(node, \"decorator_list\", []) or []:\n",
        "#             try:\n",
        "#                 decs.append(ast.unparse(d))  # py>=3.9\n",
        "#             except Exception:\n",
        "#                 decs.append(d.__class__.__name__)\n",
        "#         return decs\n",
        "\n",
        "#     def _get_args_brief(self, func_node):\n",
        "#         # خلاصه امضای تابع: فقط نام آرگومان‌ها + وجود vararg/kwarg\n",
        "#         args = []\n",
        "#         try:\n",
        "#             args = [a.arg for a in func_node.args.args]\n",
        "#         except Exception:\n",
        "#             args = []\n",
        "\n",
        "#         return {\n",
        "#             \"args\": args,\n",
        "#             \"vararg\": getattr(func_node.args, \"vararg\", None).arg if getattr(func_node.args, \"vararg\", None) else None,\n",
        "#             \"kwarg\": getattr(func_node.args, \"kwarg\", None).arg if getattr(func_node.args, \"kwarg\", None) else None,\n",
        "#         }\n",
        "\n",
        "#     def _qualify(self, name):\n",
        "#         # qualified_name حداقلی: اگر داخل کلاس باشیم: Class.method\n",
        "#         if self.class_stack:\n",
        "#             return \".\".join(self.class_stack + [name])\n",
        "#         return name\n",
        "\n",
        "#     def visit_ClassDef(self, node: ast.ClassDef):\n",
        "#         start, end = self._get_lines(node)\n",
        "#         qname = self._qualify(node.name)  # اگر nested class باشد هم پوشش می‌دهد\n",
        "\n",
        "#         extra = {\n",
        "#             \"qualified_name\": qname,\n",
        "#             \"start_line\": start,\n",
        "#             \"end_line\": end,\n",
        "#             \"docstring\": self._get_docstring(node),\n",
        "#             \"decorators\": self._get_decorators(node),\n",
        "#             \"parent_class\": self.class_stack[-1] if self.class_stack else None,\n",
        "#         }\n",
        "\n",
        "#         cid = new_node(\"class\", node.name, self.file_path, extra=extra)\n",
        "\n",
        "#         # File -> Class (contains)\n",
        "#         edges.append({\"src\": file_nodes[self.file_path], \"dst\": cid, \"type\": \"contains\"})\n",
        "\n",
        "#         # اگر کلاس داخل کلاس باشد: ParentClass -> Class (contains)\n",
        "#         if self.class_stack:\n",
        "#             parent_qname = \".\".join(self.class_stack)\n",
        "#             parent_cid = class_nodes.get((self.file_path, parent_qname))\n",
        "#             if parent_cid is not None:\n",
        "#                 edges.append({\"src\": parent_cid, \"dst\": cid, \"type\": \"contains\"})\n",
        "\n",
        "#         # ثبت کلاس با کلید دقیق (file, qualified_name)\n",
        "#         class_nodes[(self.file_path, qname)] = cid\n",
        "\n",
        "#         # ورود به کلاس و ادامه پیمایش برای متدها/کلاس‌های داخلی\n",
        "#         self.class_stack.append(node.name)\n",
        "#         self.generic_visit(node)\n",
        "#         self.class_stack.pop()\n",
        "\n",
        "#     def visit_FunctionDef(self, node: ast.FunctionDef):\n",
        "#         self._handle_function_like(node, is_async=False)\n",
        "\n",
        "#     def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef):\n",
        "#         self._handle_function_like(node, is_async=True)\n",
        "\n",
        "#     def _handle_function_like(self, node, is_async: bool):\n",
        "#         start, end = self._get_lines(node)\n",
        "#         qname = self._qualify(node.name)\n",
        "\n",
        "#         is_method = bool(self.class_stack)\n",
        "#         parent_class = self.class_stack[-1] if self.class_stack else None\n",
        "\n",
        "#         extra = {\n",
        "#             \"qualified_name\": qname,\n",
        "#             \"start_line\": start,\n",
        "#             \"end_line\": end,\n",
        "#             \"docstring\": self._get_docstring(node),\n",
        "#             \"decorators\": self._get_decorators(node),\n",
        "#             \"is_async\": is_async,\n",
        "#             \"is_method\": is_method,\n",
        "#             \"parent_class\": parent_class,\n",
        "#             **self._get_args_brief(node),\n",
        "#         }\n",
        "\n",
        "#         fid = new_node(\"function\", node.name, self.file_path, extra=extra)\n",
        "\n",
        "#         # اگر متد است: Class -> Function (contains)\n",
        "#         if is_method:\n",
        "#             parent_qname = \".\".join(self.class_stack)\n",
        "#             parent_cid = class_nodes.get((self.file_path, parent_qname))\n",
        "#             if parent_cid is not None:\n",
        "#                 edges.append({\"src\": parent_cid, \"dst\": fid, \"type\": \"contains\"})\n",
        "#             else:\n",
        "#                 # fallback اگر به هر دلیل کلاس parent ثبت نشده بود\n",
        "#                 edges.append({\"src\": file_nodes[self.file_path], \"dst\": fid, \"type\": \"contains\"})\n",
        "#         else:\n",
        "#             # تابع سطح فایل: File -> Function\n",
        "#             edges.append({\"src\": file_nodes[self.file_path], \"dst\": fid, \"type\": \"contains\"})\n",
        "\n",
        "#         # ثبت تابع با کلید دقیق (file, qualified_name)\n",
        "#         function_nodes[(self.file_path, qname)] = fid\n",
        "\n",
        "#         # ادامه پیمایش (اگر تابع داخل تابع داشته باشیم)\n",
        "#         self.generic_visit(node)\n",
        "\n",
        "\n",
        "# # -----------------------------\n",
        "# # 3) اجرای استخراج روی همه فایل‌ها\n",
        "# # -----------------------------\n",
        "# function_nodes = {}  # (file, qualified_name) -> node_id\n",
        "# class_nodes = {}     # (file, qualified_name) -> node_id\n",
        "\n",
        "# for f in tqdm(python_files):\n",
        "#     try:\n",
        "#         with open(f, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fp:\n",
        "#             src = fp.read()\n",
        "#         tree = ast.parse(src, filename=f)\n",
        "#     except Exception:\n",
        "#         continue\n",
        "\n",
        "#     visitor = CodeEntityVisitor(f)\n",
        "#     visitor.visit(tree)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hqOsN4SLJ_Si"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  ساخت یال Import\n",
        "def extract_imports(tree, file_path):\n",
        "    imports = []\n",
        "    for node in ast.walk(tree):\n",
        "        if isinstance(node, ast.Import):\n",
        "            for alias in node.names:\n",
        "                imports.append(alias.name)\n",
        "        elif isinstance(node, ast.ImportFrom):\n",
        "            mod = node.module if node.module else \"\"\n",
        "            imports.append(mod)\n",
        "    return imports\n",
        "\n",
        "\n",
        "for f in tqdm(python_files):\n",
        "    try:\n",
        "        src = open(f, \"r\", encoding=\"utf-8\").read()\n",
        "        tree = ast.parse(src)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    imports = extract_imports(tree, f)\n",
        "\n",
        "    for im in imports:\n",
        "        edges.append({\n",
        "            \"src\": file_nodes[f],\n",
        "            \"dst\": new_node(\"module\", im, f),\n",
        "            \"type\": \"import\"\n",
        "        })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtyStetrDn6A",
        "outputId": "6aa50082-5941-4d96-a84f-2daf26c221a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [00:00<00:00, 177.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  ساخت یال CALL\n",
        "\n",
        "def get_func_name(call_node):\n",
        "    \"\"\"استخراج نام تابع\"\"\"\n",
        "    if isinstance(call_node.func, ast.Name):\n",
        "        return call_node.func.id\n",
        "    elif isinstance(call_node.func, ast.Attribute):\n",
        "        return call_node.func.attr\n",
        "    return None\n",
        "\n",
        "\n",
        "for f in tqdm(python_files):\n",
        "    try:\n",
        "        src = open(f, \"r\", encoding=\"utf-8\").read()\n",
        "        tree = ast.parse(src)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    # پیدا کردن تابع فعلی برای ساخت یال call\n",
        "    current_func_node_id = None\n",
        "\n",
        "    for node in ast.walk(tree):\n",
        "\n",
        "        # موقع ورود به تابع\n",
        "        if isinstance(node, ast.FunctionDef):\n",
        "            current_func_node_id = function_nodes.get((f, node.name), None)\n",
        "\n",
        "        # فراخوانی تابع\n",
        "        if isinstance(node, ast.Call) and current_func_node_id is not None:\n",
        "            callee = get_func_name(node)\n",
        "\n",
        "            # پیدا کردن callee در پروژه (در فایل‌های دیگر)\n",
        "            callee_node_id = None\n",
        "            for (file2, name2), nid in function_nodes.items():\n",
        "                if name2 == callee:\n",
        "                    callee_node_id = nid\n",
        "                    break\n",
        "\n",
        "            if callee_node_id:\n",
        "                edges.append({\n",
        "                    \"src\": current_func_node_id,\n",
        "                    \"dst\": callee_node_id,\n",
        "                    \"type\": \"call\"\n",
        "                })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HkA3t7mDqo9",
        "outputId": "1a0a582d-9487-4784-eb3b-0fe79be64703"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 90/90 [00:00<00:00, 103.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ذخیره خروجی به صورت JSON\n",
        "with open(\"/content/nodes.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(nodes, f, indent=2)\n",
        "\n",
        "with open(\"/content/edges.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(edges, f, indent=2)\n",
        "\n",
        "print(\"Done! فایل‌ها تولید شدند: nodes.json و edges.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEfBnYtbD1Ci",
        "outputId": "53e4fa13-d383-4c59-be38-09971cd3e794"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! فایل‌ها تولید شدند: nodes.json و edges.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "nodes = json.load(open(\"/content/nodes.json\"))\n",
        "node_types = {n[\"type\"] for n in nodes}\n",
        "print(node_types)\n",
        "\n",
        "\n",
        "files = [n for n in nodes if n[\"type\"] == \"file\"]\n",
        "classes = [n for n in nodes if n[\"type\"] == \"class\"]\n",
        "funcs = [n for n in nodes if n[\"type\"] == \"function\"]\n",
        "Module = [n for n in nodes if n[\"type\"] == \"module\"]\n",
        "\n",
        "print(len(files), len(classes), len(funcs), len(Module))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybMMuYCxEnH8",
        "outputId": "7e9d7ff1-7855-47e9-9cfe-4430446907d4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'class', 'function', 'file', 'module'}\n",
            "90 237 761 526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvis\n",
        "!pip -q install --upgrade pyvis jinja2\n",
        "\n",
        "\n",
        "from pyvis.network import Network\n",
        "import json\n",
        "\n",
        "nodes = json.load(open(\"/content/nodes.json\", \"r\", encoding=\"utf-8\"))\n",
        "edges = json.load(open(\"/content/edges.json\", \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "net = Network(height=\"800px\", width=\"100%\", bgcolor=\"white\", directed=True, notebook=True)\n",
        "\n",
        "for n in nodes:\n",
        "    net.add_node(n[\"id\"], label=n.get(\"name\",\"\"), title=n.get(\"type\",\"\"), group=n.get(\"type\",\"unknown\"))\n",
        "\n",
        "for e in edges:\n",
        "    net.add_edge(e[\"src\"], e[\"dst\"], title=e.get(\"type\",\"\"), label=e.get(\"type\",\"\"))\n",
        "\n",
        "net.write_html(\"/content/code_graph.html\")\n",
        "print(\"Saved: /content/code_graph.html\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LYSHCKVeftIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c9aa88-42d4-4de2-c52f-7fc75cc9d22c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvis in /usr/local/lib/python3.12/dist-packages (0.3.2)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.12/dist-packages (from pyvis) (3.1.6)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from pyvis) (4.1.1)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.12/dist-packages (from pyvis) (3.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.9.6->pyvis) (3.0.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.6.0)\n",
            "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
            "Saved: /content/code_graph.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGpt Request"
      ],
      "metadata": {
        "id": "CY-qk7x6f5wQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken openai tqdm\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "upacj6EWRXqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebca9ab-7b68-4512-851e-c618ea080a43"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.13.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os, ast\n",
        "from tqdm import tqdm\n",
        "\n",
        "nodes = json.load(open(\"/content/nodes.json\"))\n",
        "\n",
        "def load_code(file_path):\n",
        "    try:\n",
        "        return open(file_path, \"r\", encoding=\"utf-8\").read()\n",
        "    except:\n",
        "        return \"\"\n"
      ],
      "metadata": {
        "id": "R8JXMLI8RaEQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import os\n",
        "\n",
        "def extract_node_source(file_path, node_name, node_type):\n",
        "    \"\"\"\n",
        "    از فایل، فقط بدنه گره (class/function) را استخراج می‌کند.\n",
        "    اگر پیدا نشود، کل فایل را برمی‌گرداند.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        src = open(file_path, \"r\", encoding=\"utf-8\").read()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        tree = ast.parse(src)\n",
        "    except:\n",
        "        return src  # fallback\n",
        "\n",
        "    for node in ast.walk(tree):\n",
        "        # استخراج بدنه تابع\n",
        "        if node_type == \"function\" and isinstance(node, ast.FunctionDef):\n",
        "            if node.name == node_name:\n",
        "                start = node.lineno - 1\n",
        "                end = node.end_lineno\n",
        "                return \"\\n\".join(src.splitlines()[start:end])\n",
        "\n",
        "        # استخراج بدنه کلاس\n",
        "        if node_type == \"class\" and isinstance(node, ast.ClassDef):\n",
        "            if node.name == node_name:\n",
        "                start = node.lineno - 1\n",
        "                end = node.end_lineno\n",
        "                return \"\\n\".join(src.splitlines()[start:end])\n",
        "\n",
        "    # اگر هیچ‌کدام پیدا نشد → کل فایل را برگردان\n",
        "    return src\n",
        "\n",
        "\n",
        "def build_embedding_text(node):\n",
        "    \"\"\"\n",
        "    متن کامل و بهینه برای مدل embedding را می‌سازد.\n",
        "    ساختار:\n",
        "    - نوع گره\n",
        "    - نام\n",
        "    - مسیر فایل\n",
        "    - بخشی از کد\n",
        "    \"\"\"\n",
        "\n",
        "    file_path = node[\"file\"]\n",
        "    name = node[\"name\"]\n",
        "    node_type = node[\"type\"]\n",
        "\n",
        "    # بهترین حالت: فقط بدنه گره\n",
        "    code_snippet = extract_node_source(file_path, name, node_type)\n",
        "\n",
        "    text = f\"\"\"\n",
        "### NODE INFORMATION ###\n",
        "Type: {node_type.upper()}\n",
        "Name: {name}\n",
        "File: {file_path}\n",
        "\n",
        "### CONTEXT ###\n",
        "This node comes from a Python project.\n",
        "Provide a semantic representation of its behavior and responsibilities.\n",
        "\n",
        "### SOURCE CODE ###\n",
        "{code_snippet}\n",
        "\n",
        "### END ###\n",
        "    \"\"\"\n",
        "\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "H34tazYWRaLZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT Embeddings Function\n",
        "\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "METIS_KEY = \"tpsg-sH8SGowLyM3C6xDkKCpbuNlWhkqW8MB\"\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key = METIS_KEY,\n",
        "    base_url = \"https://api.tapsage.com/openai/v1\"\n",
        ")\n",
        "def get_embedding(text, max_len=5000, retry_count=5):\n",
        "    \"\"\"\n",
        "    گرفتن embedding از مدل GPT با مدیریت خطا و retry.\n",
        "    \"\"\"\n",
        "\n",
        "    # کوتاه‌سازی متن برای جلوگیری از خطا\n",
        "    text = text[:max_len]\n",
        "\n",
        "    for attempt in range(retry_count):\n",
        "        try:\n",
        "            response = client.embeddings.create(\n",
        "                model=\"text-embedding-3-large\",\n",
        "                input=text\n",
        "            )\n",
        "            return response.data[0].embedding\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error: {e}\")\n",
        "            print(f\"Retrying ({attempt+1}/{retry_count})...\")\n",
        "            time.sleep(2)\n",
        "\n",
        "    print(\"❌ Failed to get embedding after retries.\")\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "vpFzfNyLSnrQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "nodes = json.load(open(\"/content/nodes.json\", \"r\", encoding=\"utf-8\"))\n",
        "edges = json.load(open(\"/content/edges.json\", \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "node_by_id = {n[\"id\"]: n for n in nodes}\n",
        "\n",
        "adj = defaultdict(list)\n",
        "for e in edges:\n",
        "    s, d, t = e[\"src\"], e[\"dst\"], e[\"type\"]\n",
        "    adj[s].append((t, d, \"out\"))\n",
        "    adj[d].append((t, s, \"in\"))\n",
        "\n",
        "def build_embedding_text_with_edges(node, max_neighbors=25):\n",
        "    \"\"\"\n",
        "    Build embedding text using:\n",
        "    - node metadata\n",
        "    - code snippet (from your existing build_embedding_text(node))\n",
        "    - small local graph context (neighbors + edge types)\n",
        "    \"\"\"\n",
        "    base_text = build_embedding_text(node)\n",
        "\n",
        "    nid = node[\"id\"]\n",
        "    neighbors = adj.get(nid, [])[:max_neighbors]\n",
        "\n",
        "    edge_lines = []\n",
        "    for etype, nb_id, direction in neighbors:\n",
        "        nb = node_by_id.get(nb_id)\n",
        "        if nb is None:\n",
        "            continue\n",
        "        edge_lines.append(\n",
        "            f\"- ({direction}) {etype} -> [{nb['type']}] {nb['name']}\"\n",
        "        )\n",
        "\n",
        "    edge_context = \"\\n\".join(edge_lines) if edge_lines else \"None\"\n",
        "\n",
        "    return f\"\"\"{base_text}\n",
        "\n",
        "### LOCAL GRAPH CONTEXT (1-hop) ###\n",
        "Node id: {nid}\n",
        "Neighbors (limited to {max_neighbors}):\n",
        "{edge_context}\n",
        "\"\"\".strip()\n",
        "\n",
        "node_embeddings = {}\n",
        "\n",
        "for node in tqdm(nodes, desc=\"Embedding nodes+edges\"):\n",
        "    text = build_embedding_text_with_edges(node, max_neighbors=25)\n",
        "    emb = get_embedding(text)\n",
        "\n",
        "    if emb is None:\n",
        "        print(f\"Skipped node {node['id']} due to embedding failure.\")\n",
        "        continue\n",
        "\n",
        "    node_embeddings[str(node[\"id\"])] = emb\n",
        "\n",
        "# Save\n",
        "out_path = \"/content/node_embeddings.json\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(node_embeddings, f)\n",
        "\n",
        "print(\"DONE! Embeddings saved to:\", out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvdjw-3xSoaZ",
        "outputId": "d6d511a8-0a1c-47c4-9d37-e90b58a282fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding nodes+edges:  20%|█▉        | 318/1614 [19:01<59:12,  2.74s/it]  "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "path = \"/content/node_embeddings.json\"\n",
        "emb = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "print(\"Total nodes embedded:\", len(emb))\n",
        "print(\"First 10 node ids:\", list(emb.keys())[:10])\n",
        "\n",
        "sample_id = list(emb.keys())[10]\n",
        "print(\"Sample node id:\", sample_id)\n",
        "print(\"Vector length:\", len(emb[sample_id]))\n",
        "print(\"First 10 dims:\", emb[sample_id][:10])\n"
      ],
      "metadata": {
        "id": "tSrCLzMDEZhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "emb = json.load(open(\"/content/node_embeddings.json\"))\n",
        "nodes = json.load(open(\"/content/nodes.json\", \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "lengths = set(len(v) for v in emb.values())\n",
        "print(lengths)\n",
        "\n",
        "emb_list = list(emb.values())\n",
        "unique_count = len({tuple(v) for v in emb_list})\n",
        "print(\"Unique vectors:\", unique_count, \"/\", len(emb_list))\n",
        "\n",
        "\n",
        "sample = list(emb.values())[:50]\n",
        "sim = cosine_similarity(sample)\n",
        "\n",
        "print(\"Mean similarity:\", sim.mean())\n",
        "print(\"Max similarity:\", sim.max())\n",
        "print(\"Min similarity:\", sim.min())\n",
        "bad_nodes = []\n",
        "\n",
        "for node in nodes:\n",
        "    text = build_embedding_text(node)\n",
        "    if len(text.strip()) < 20:\n",
        "        bad_nodes.append(node[\"id\"])\n",
        "\n",
        "len(bad_nodes), bad_nodes[:10]\n",
        "\n",
        "\n",
        "print(\"Bad nodes number\", len(bad_nodes))\n",
        "target_id = \"39\" #Random Node For test\n",
        "\n",
        "target = np.array(emb[target_id]).reshape(1, -1)\n",
        "all_vecs = np.array(list(emb.values()))\n",
        "node_ids = list(emb.keys())\n",
        "\n",
        "sims = cosine_similarity(target, all_vecs)[0]\n",
        "top = np.argsort(sims)[::-1][:10]\n",
        "\n",
        "for i in top:\n",
        "    print(node_ids[i], sims[i])\n"
      ],
      "metadata": {
        "id": "BaFNzBbdyzKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "41XehQCZRdD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNh--FmsSRnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uUcUa84ISTVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vy56nJljSU5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = range(4, 30)\n",
        "\n",
        "results = []\n",
        "\n",
        "for k in range(8, 30):\n",
        "    # --- clustering ---\n",
        "    labels = cop_kmeans(X_gnn, k, must, cannot, max_iter=100, seed=42)\n",
        "    labels = fill_unassigned_by_nearest_center(X_gnn, labels, k)\n",
        "\n",
        "    # --- evaluation on HELD-OUT edges ---\n",
        "    coh, cou, used, intra, inter = cohesion_coupling_from_edges(\n",
        "        labels, test_edges, global_to_local_func\n",
        "    )\n",
        "\n",
        "    score = coh - cou   # λ = 1\n",
        "\n",
        "    cluster_sizes = [np.sum(labels == i) for i in range(k)]\n",
        "\n",
        "    results.append({\n",
        "        \"k\": k,\n",
        "        \"cohesion\": coh,\n",
        "        \"coupling\": cou,\n",
        "        \"score\": score,\n",
        "        \"min_size\": min(cluster_sizes),\n",
        "        \"max_size\": max(cluster_sizes),\n",
        "    })\n",
        "    best_score = max(r[\"score\"] for r in results)\n",
        "\n",
        "candidates = [\n",
        "    r for r in results\n",
        "    if r[\"score\"] >= 0.95 * best_score\n",
        "    and r[\"min_size\"] >= 5\n",
        "    and r[\"max_size\"] <= 0.5 * len(function_nodes)\n",
        "]\n",
        "\n",
        "best_k = min(r[\"k\"] for r in candidates)\n",
        "print(\"Best k:\", best_k)\n"
      ],
      "metadata": {
        "id": "olsvy592SWcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GNN Training"
      ],
      "metadata": {
        "id": "wHZFQKMyFuu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torch-geometric scikit-learn -q\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from torch_geometric.data import HeteroData\n",
        "from collections import defaultdict\n",
        "\n",
        "NODES_PATH = \"/content/nodes.json\"\n",
        "EDGES_PATH = \"/content/edges.json\"\n",
        "EMB_PATH   = \"/content/node_embeddings.json\"\n",
        "\n",
        "nodes = json.load(open(NODES_PATH, \"r\", encoding=\"utf-8\"))\n",
        "edges = json.load(open(EDGES_PATH, \"r\", encoding=\"utf-8\"))\n",
        "emb  = json.load(open(EMB_PATH, \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "# node id -> type\n",
        "id_to_type = {n[\"id\"]: n[\"type\"] for n in nodes}\n",
        "\n",
        "# group ids per type\n",
        "type_to_ids = defaultdict(list)\n",
        "for n in nodes:\n",
        "    type_to_ids[n[\"type\"]].append(n[\"id\"])\n",
        "\n",
        "# local index per type\n",
        "local_index = {t: {nid:i for i, nid in enumerate(ids)} for t, ids in type_to_ids.items()}\n",
        "\n",
        "# embedding dim\n",
        "first_key = next(iter(emb.keys()))\n",
        "emb_dim = len(emb[first_key])\n",
        "\n",
        "data = HeteroData()\n",
        "\n",
        "# node features per type\n",
        "for t, ids in type_to_ids.items():\n",
        "    x = torch.zeros((len(ids), emb_dim), dtype=torch.float32)\n",
        "    for nid in ids:\n",
        "        v = emb.get(str(nid))\n",
        "        if v is not None:\n",
        "            x[local_index[t][nid]] = torch.tensor(v, dtype=torch.float32)\n",
        "    data[t].x = x\n",
        "\n",
        "# edges per relation\n",
        "edge_dict = defaultdict(list)\n",
        "\n",
        "for e in edges:\n",
        "    et = e[\"type\"]\n",
        "    s, d = e[\"src\"], e[\"dst\"]\n",
        "    if s not in id_to_type or d not in id_to_type:\n",
        "        continue\n",
        "    st, dt = id_to_type[s], id_to_type[d]\n",
        "\n",
        "    if et == \"contains\":\n",
        "        if st == \"file\" and dt in (\"class\", \"function\"):\n",
        "            rel = (\"file\", \"contains\", dt)\n",
        "        elif st == \"class\" and dt == \"function\":\n",
        "            rel = (\"class\", \"contains\", \"function\")\n",
        "        else:\n",
        "            continue\n",
        "    elif et == \"import\":\n",
        "        if st == \"file\" and dt == \"module\":\n",
        "            rel = (\"file\", \"imports\", \"module\")\n",
        "        else:\n",
        "            continue\n",
        "    elif et == \"call\":\n",
        "        if st == \"function\" and dt == \"function\":\n",
        "            rel = (\"function\", \"calls\", \"function\")\n",
        "        else:\n",
        "            continue\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    edge_dict[rel].append((local_index[st][s], local_index[dt][d]))\n",
        "\n",
        "for rel, pairs in edge_dict.items():\n",
        "    src, dst = zip(*pairs) if pairs else ([], [])\n",
        "    data[rel].edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
        "\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "Y2T2db5K4ZG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import HeteroConv, SAGEConv\n",
        "\n",
        "class HeteroRGCN(nn.Module):\n",
        "    def __init__(self, metadata, in_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.node_lin = nn.ModuleDict({t: nn.Linear(in_dim, hidden_dim) for t in metadata[0]})\n",
        "\n",
        "        self.conv1 = HeteroConv({et: SAGEConv((-1, -1), hidden_dim) for et in metadata[1]})\n",
        "        self.conv2 = HeteroConv({et: SAGEConv((-1, -1), out_dim)    for et in metadata[1]})\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        x0 = {t: self.node_lin[t](x) for t, x in x_dict.items()}\n",
        "\n",
        "        x1 = self.conv1(x0, edge_index_dict)\n",
        "        for t in x0:\n",
        "            x1[t] = x0[t] if x1.get(t) is None else F.relu(x1[t])\n",
        "\n",
        "        x2 = self.conv2(x1, edge_index_dict)\n",
        "        for t in x1:\n",
        "            x2[t] = x1[t] if x2.get(t) is None else x2[t]\n",
        "\n",
        "        return x2"
      ],
      "metadata": {
        "id": "OzHgzRF7aK3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.utils import negative_sampling\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = data.to(device)\n",
        "\n",
        "edge_type = (\"function\", \"calls\", \"function\")\n",
        "edge_index = data[edge_type].edge_index\n",
        "m = edge_index.size(1)\n",
        "\n",
        "perm = torch.randperm(m, device=device)\n",
        "train_m = int(0.9 * m)\n",
        "train_ei = edge_index[:, perm[:train_m]]\n",
        "val_ei   = edge_index[:, perm[train_m:]]\n",
        "\n",
        "def edge_score(z_src, z_dst, ei):\n",
        "    return (z_src[ei[0]] * z_dst[ei[1]]).sum(dim=-1)\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def loss_fn(model, pos_ei):\n",
        "    model.train()\n",
        "    z = model(data.x_dict, data.edge_index_dict)\n",
        "    zf = z[\"function\"]\n",
        "\n",
        "    pos = edge_score(zf, zf, pos_ei)\n",
        "    pos_y = torch.ones(pos.size(0), device=device)\n",
        "\n",
        "    neg_ei = negative_sampling(pos_ei, num_nodes=zf.size(0), num_neg_samples=pos_ei.size(1))\n",
        "    neg = edge_score(zf, zf, neg_ei)\n",
        "    neg_y = torch.zeros(neg.size(0), device=device)\n",
        "\n",
        "    s = torch.cat([pos, neg])\n",
        "    y = torch.cat([pos_y, neg_y])\n",
        "    return bce(s, y)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_auc(model, pos_ei, num_neg=5000):\n",
        "    model.eval()\n",
        "    z = model(data.x_dict, data.edge_index_dict)\n",
        "    zf = z[\"function\"]\n",
        "\n",
        "    pos = edge_score(zf, zf, pos_ei)\n",
        "    pos_y = torch.ones(pos.size(0))\n",
        "\n",
        "    neg_ei = negative_sampling(pos_ei, num_nodes=zf.size(0), num_neg_samples=min(num_neg, pos_ei.size(1)))\n",
        "    neg = edge_score(zf, zf, neg_ei)\n",
        "    neg_y = torch.zeros(neg.size(0))\n",
        "\n",
        "    y = torch.cat([pos_y, neg_y]).cpu().numpy()\n",
        "    s = torch.cat([pos, neg]).cpu().numpy()\n",
        "    return roc_auc_score(y, s)\n",
        "\n",
        "in_dim = data[\"function\"].x.size(1)\n",
        "model = HeteroRGCN(data.metadata(), in_dim=in_dim, hidden_dim=256, out_dim=128).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "best_auc = 0.0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    loss = loss_fn(model, train_ei)\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if epoch == 1 or epoch % 10 == 0:\n",
        "        auc = eval_auc(model, val_ei)\n",
        "        print(f\"Epoch {epoch:03d} | Loss {loss.item():.4f} | Val AUC {auc:.4f}\")\n",
        "        if auc > best_auc:\n",
        "            best_auc = auc\n",
        "            best_state = deepcopy(model.state_dict())\n",
        "\n",
        "if best_state:\n",
        "    model.load_state_dict(best_state)\n",
        "print(\"Best Val AUC:\", best_auc)\n"
      ],
      "metadata": {
        "id": "6oS4c1elVeN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def export_embeddings(model, out_path=\"/content/gnn_embeddings.json\"):\n",
        "    model.eval()\n",
        "    z = model(data.x_dict, data.edge_index_dict)\n",
        "    z_func = z[\"function\"].detach().cpu().numpy()\n",
        "    out = {str(i): z_func[i].tolist() for i in range(z_func.shape[0])}\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(out, f)\n",
        "    print(\"Saved:\", out_path, \"shape:\", z_func.shape)\n",
        "\n",
        "export_embeddings(model)\n"
      ],
      "metadata": {
        "id": "4yXb3qDsVsTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "@torch.no_grad()\n",
        "# def eval_hits_mrr(model, data, pos_ei, K=15, num_neg_per_pos=50):\n",
        "def eval_hits_mrr(model, data, pos_ei, K=10, num_neg_per_pos=50):\n",
        "    model.eval()\n",
        "    z = model(data.x_dict, data.edge_index_dict)[\"function\"]  # [N, d]\n",
        "    N = z.size(0)\n",
        "\n",
        "    # scores for positives\n",
        "    pos_scores = (z[pos_ei[0]] * z[pos_ei[1]]).sum(dim=-1)\n",
        "\n",
        "    # build negatives per positive by corrupting destination\n",
        "    # for each pos (u,v), sample v' negatives\n",
        "    ranks = []\n",
        "    hits = 0\n",
        "\n",
        "    for i in range(pos_ei.size(1)):\n",
        "        u = pos_ei[0, i]\n",
        "        v = pos_ei[1, i]\n",
        "\n",
        "        neg_vs = torch.randint(0, N, (num_neg_per_pos,), device=z.device)\n",
        "        # scores: positive + negatives\n",
        "        s_pos = (z[u] * z[v]).sum()\n",
        "        s_neg = (z[u].unsqueeze(0) * z[neg_vs]).sum(dim=-1)\n",
        "\n",
        "        # rank of positive among (neg + pos), higher is better\n",
        "        all_scores = torch.cat([s_neg, s_pos.view(1)], dim=0)\n",
        "        # descending rank: 1 is best\n",
        "        rank = (all_scores > s_pos).sum().item() + 1\n",
        "        ranks.append(rank)\n",
        "        if rank <= K:\n",
        "            hits += 1\n",
        "\n",
        "    mrr = sum(1.0/r for r in ranks) / len(ranks)\n",
        "    hits_at_k = hits / len(ranks)\n",
        "    return hits_at_k, mrr\n",
        "\n",
        "hits10, mrr = eval_hits_mrr(model, data, val_ei, K=15)\n",
        "# hits10, mrr = eval_hits_mrr(model, data, val_ei, K=10)\n",
        "print(\"Hits@10:\", hits10, \"MRR:\", mrr)"
      ],
      "metadata": {
        "id": "ow4nFvLAgP2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "data_train = deepcopy(data)\n",
        "data_train[edge_type].edge_index = train_ei\n"
      ],
      "metadata": {
        "id": "A2DIa9X2g1Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.utils import negative_sampling\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_auc_llm_only(data_train, val_ei, edge_type):\n",
        "    # Use raw input features directly\n",
        "    z = data_train['function'].x\n",
        "    pos = (z[val_ei[0]] * z[val_ei[1]]).sum(dim=-1)\n",
        "\n",
        "    neg_ei = negative_sampling(val_ei, num_nodes=z.size(0), num_neg_samples=val_ei.size(1))\n",
        "    neg = (z[neg_ei[0]] * z[neg_ei[1]]).sum(dim=-1)\n",
        "\n",
        "    y = torch.cat([torch.ones_like(pos), torch.zeros_like(neg)]).cpu().numpy()\n",
        "    s = torch.cat([pos, neg]).cpu().numpy()\n",
        "    return roc_auc_score(y, s)\n",
        "\n",
        "llm_auc = eval_auc_llm_only(data_train, val_ei, edge_type)\n",
        "print(\"LLM-only AUC:\", llm_auc)\n"
      ],
      "metadata": {
        "id": "Q1_-9ESVkI4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Step clustring**"
      ],
      "metadata": {
        "id": "f1Fb1jawnrmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constraint-Based Clustering\n"
      ],
      "metadata": {
        "id": "Aj2bZmpoB8Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Helpers\n",
        "# ----------------------------\n",
        "def l2_normalize(X, eps=1e-12):\n",
        "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
        "    return X / (norms + eps)\n",
        "\n",
        "def safe_edge_fields(e):\n",
        "    src = e.get(\"src\", e.get(\"source\", e.get(\"from\")))\n",
        "    dst = e.get(\"dst\", e.get(\"target\", e.get(\"to\")))\n",
        "    et  = e.get(\"type\", e.get(\"label\", \"\"))\n",
        "    w   = e.get(\"weight\", e.get(\"count\", 1))\n",
        "    return src, dst, et, w\n",
        "\n",
        "def cosine_sim_pairs(X_norm, pairs):\n",
        "    # X_norm must be L2-normalized\n",
        "    return np.array([float(np.dot(X_norm[i], X_norm[j])) for (i, j) in pairs], dtype=np.float32)"
      ],
      "metadata": {
        "id": "paPpirqfB-yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expected node schema: {\"id\": ..., \"type\": \"function\", ...}\n",
        "function_nodes = [n for n in nodes if str(n.get(\"type\", \"\")).lower() == \"function\"]\n",
        "if len(function_nodes) == 0:\n",
        "    raise ValueError(\"No function nodes found. Check nodes.json schema: node['type'] must be 'function'.\")\n",
        "\n",
        "func_global_ids = [fn[\"id\"] for fn in function_nodes]\n",
        "global_to_local_func = {gid: i for i, gid in enumerate(func_global_ids)}"
      ],
      "metadata": {
        "id": "PkjuP14CCByU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# 1) Set the path to your embeddings file (adjust if needed)\n",
        "GNN_EMB_PATH = \"/content/gnn_embeddings.json\"\n",
        "\n",
        "if not os.path.exists(GNN_EMB_PATH):\n",
        "    # try common alternatives\n",
        "    candidates = [\n",
        "        \"/content/drive/MyDrive/gnn_embeddings.json\",\n",
        "        \"/content/gnn_embeddings (1).json\",\n",
        "        \"/content/drive/MyDrive/gnn_embeddings (1).json\",\n",
        "    ]\n",
        "    found = None\n",
        "    for p in candidates:\n",
        "        if os.path.exists(p):\n",
        "            found = p\n",
        "            break\n",
        "    if found is None:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Cannot find gnn_embeddings.json. Checked: {GNN_EMB_PATH} and {candidates}\"\n",
        "        )\n",
        "    GNN_EMB_PATH = found\n",
        "\n",
        "with open(GNN_EMB_PATH, \"r\") as f:\n",
        "    gnn_embeddings = json.load(f)\n",
        "\n",
        "print(\"Loaded gnn_embeddings from:\", GNN_EMB_PATH)\n",
        "print(\"Type:\", type(gnn_embeddings))\n",
        "if isinstance(gnn_embeddings, dict):\n",
        "    print(\"Dict keys sample:\", list(gnn_embeddings.keys())[:5])\n",
        "elif isinstance(gnn_embeddings, list):\n",
        "    print(\"List length:\", len(gnn_embeddings), \"sample item keys:\", list(gnn_embeddings[0].keys()) if len(gnn_embeddings) else None)\n"
      ],
      "metadata": {
        "id": "rFHozGyFFRdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_X_from_embeddings(emb, func_ids):\n",
        "    # dict keyed by id\n",
        "    if isinstance(emb, dict):\n",
        "        # try both raw key and string key\n",
        "        def get_vec(gid):\n",
        "            if gid in emb: return emb[gid]\n",
        "            if str(gid) in emb: return emb[str(gid)]\n",
        "            raise KeyError(gid)\n",
        "\n",
        "        X = np.array([get_vec(gid) for gid in func_ids], dtype=np.float32)\n",
        "        return X\n",
        "\n",
        "    # list of objects\n",
        "    if isinstance(emb, list):\n",
        "        id_to_vec = {}\n",
        "        for item in emb:\n",
        "            # try common fields\n",
        "            gid = item.get(\"id\", item.get(\"_id\", item.get(\"node_id\")))\n",
        "            vec = item.get(\"embedding\", item.get(\"emb\", item.get(\"vector\")))\n",
        "            if gid is not None and vec is not None:\n",
        "                id_to_vec[gid] = vec\n",
        "                id_to_vec[str(gid)] = vec\n",
        "        X = np.array([id_to_vec.get(gid, id_to_vec.get(str(gid))) for gid in func_ids], dtype=np.float32)\n",
        "        if np.any([v is None for v in X]):\n",
        "            missing = [gid for gid in func_ids if (gid not in id_to_vec and str(gid) not in id_to_vec)]\n",
        "            raise KeyError(f\"Missing embeddings for {len(missing)} function ids. Example missing: {missing[:5]}\")\n",
        "        return X\n",
        "\n",
        "    raise TypeError(\"Unsupported embeddings schema. Must be dict or list.\")\n",
        "\n",
        "X_gnn = np.array([gnn_embeddings[str(i)] for i in range(len(function_nodes))], dtype=np.float32)\n",
        "if X_gnn.ndim != 2 or X_gnn.shape[0] != len(func_global_ids):\n",
        "    raise ValueError(f\"Bad X_gnn shape: {X_gnn.shape}. Expected (num_functions, dim).\")\n",
        "\n",
        "print(\"X_gnn:\", X_gnn.shape)\n",
        "\n",
        "# If you do NOT have X_llm in your notebook, do not reference labels_llm anywhere.\n",
        "HAS_LLM = \"X_llm\" in globals() and isinstance(globals()[\"X_llm\"], np.ndarray)\n"
      ],
      "metadata": {
        "id": "W-IPZofqCEfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_constraints(function_nodes, edges, global_to_local_func, X_norm,\n",
        "                      tau_must=0.90,\n",
        "    tau_cannot=0.00,\n",
        "    max_cannot=2000,\n",
        "    use_same_file_must=False):\n",
        "# def build_constraints(function_nodes, edges, global_to_local_func, X_norm,\n",
        "#                       tau_must=0.85, tau_cannot=0.10,\n",
        "#                       max_cannot=20000,\n",
        "#                       use_same_file_must=True):\n",
        "\n",
        "    must = set()\n",
        "    cannot = set()\n",
        "\n",
        "# Xg_norm = l2_normalize(X_gnn)\n",
        "# must, cannot = build_constraints(\n",
        "#     function_nodes=function_nodes,\n",
        "#     edges=train_edges,\n",
        "#     global_to_local_func=global_to_local_func,\n",
        "#     X_norm=Xg_norm,\n",
        "#     tau_must=0.90,\n",
        "#     tau_cannot=0.00,\n",
        "#     max_cannot=2000,\n",
        "#     use_same_file_must=False\n",
        "# )\n",
        "    # Must-link from function->function edges (calls/relations)\n",
        "    for e in edges:\n",
        "        src, dst, et, w = safe_edge_fields(e)\n",
        "        if src in global_to_local_func and dst in global_to_local_func:\n",
        "            i = global_to_local_func[src]\n",
        "            j = global_to_local_func[dst]\n",
        "            if i != j:\n",
        "                must.add((min(i, j), max(i, j)))\n",
        "\n",
        "    # Optional Must-link from same file (light sampling)\n",
        "    if use_same_file_must:\n",
        "        file_to_funcs = defaultdict(list)\n",
        "        for fn in function_nodes:\n",
        "            gid = fn[\"id\"]\n",
        "            if gid in global_to_local_func:\n",
        "                file_to_funcs[fn.get(\"file\", \"\")].append(global_to_local_func[gid])\n",
        "\n",
        "        for f, idxs in file_to_funcs.items():\n",
        "            if not f or len(idxs) < 2:\n",
        "                continue\n",
        "            idxs = sorted(set(idxs))\n",
        "            # sample adjacent pairs only (prevents explosion)\n",
        "            for a in range(min(len(idxs) - 1, 50)):\n",
        "                i, j = idxs[a], idxs[a + 1]\n",
        "                must.add((min(i, j), max(i, j)))\n",
        "\n",
        "    # Cannot-link by sampling low-cosine pairs (avoid must pairs)\n",
        "    n = X_norm.shape[0]\n",
        "    pairs = set()\n",
        "    trials = 0\n",
        "    target_pairs = min(max_cannot * 3, n * n // 50 + 1)\n",
        "\n",
        "    while len(pairs) < target_pairs and trials < target_pairs * 3:\n",
        "        i = random.randrange(n)\n",
        "        j = random.randrange(n)\n",
        "        if i == j:\n",
        "            trials += 1\n",
        "            continue\n",
        "        a, b = (i, j) if i < j else (j, i)\n",
        "        if (a, b) in must:\n",
        "            trials += 1\n",
        "            continue\n",
        "        pairs.add((a, b))\n",
        "        trials += 1\n",
        "\n",
        "    pairs_list = list(pairs)\n",
        "    sims = cosine_sim_pairs(X_norm, pairs_list)\n",
        "    low_idx = np.where(sims <= tau_cannot)[0]\n",
        "\n",
        "    if len(low_idx) > max_cannot:\n",
        "        low_idx = np.random.choice(low_idx, size=max_cannot, replace=False)\n",
        "\n",
        "    for idx in low_idx:\n",
        "        cannot.add(pairs_list[idx])\n",
        "\n",
        "    return must, cannot\n"
      ],
      "metadata": {
        "id": "_TWhlVe5CYiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def violates_constraints(i, c, labels, must_adj, cannot_adj):\n",
        "    for j in must_adj[i]:\n",
        "        lj = labels[j]\n",
        "        if lj != -1 and lj != c:\n",
        "            return True\n",
        "    for j in cannot_adj[i]:\n",
        "        lj = labels[j]\n",
        "        if lj != -1 and lj == c:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def cop_kmeans(X, k, must, cannot, max_iter=50, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n, d = X.shape\n",
        "\n",
        "    must_adj = defaultdict(set)\n",
        "    cannot_adj = defaultdict(set)\n",
        "    for i, j in must:\n",
        "        must_adj[i].add(j); must_adj[j].add(i)\n",
        "    for i, j in cannot:\n",
        "        cannot_adj[i].add(j); cannot_adj[j].add(i)\n",
        "\n",
        "    centers = X[rng.choice(n, size=k, replace=False)].copy()\n",
        "    labels = np.full(n, -1, dtype=np.int64)\n",
        "\n",
        "    for it in range(max_iter):\n",
        "        changed = False\n",
        "\n",
        "        order = rng.permutation(n)\n",
        "        for i in order:\n",
        "            dists = np.linalg.norm(X[i] - centers, axis=1)\n",
        "            for c in np.argsort(dists):\n",
        "                if not violates_constraints(i, c, labels, must_adj, cannot_adj):\n",
        "                    if labels[i] != c:\n",
        "                        labels[i] = c\n",
        "                        changed = True\n",
        "                    break\n",
        "            else:\n",
        "                labels[i] = -1  # no feasible cluster\n",
        "\n",
        "        new_centers = centers.copy()\n",
        "        for c in range(k):\n",
        "            idx = np.where(labels == c)[0]\n",
        "            if len(idx) > 0:\n",
        "                new_centers[c] = X[idx].mean(axis=0)\n",
        "            else:\n",
        "                new_centers[c] = X[rng.integers(0, n)]\n",
        "        centers = new_centers\n",
        "\n",
        "        if not changed:\n",
        "            break\n",
        "\n",
        "    return labels\n",
        "\n",
        "def constraint_violation_rate(labels, must, cannot):\n",
        "    v = 0\n",
        "    for i, j in must:\n",
        "        if labels[i] != -1 and labels[j] != -1 and labels[i] != labels[j]:\n",
        "            v += 1\n",
        "    for i, j in cannot:\n",
        "        if labels[i] != -1 and labels[j] != -1 and labels[i] == labels[j]:\n",
        "            v += 1\n",
        "    total = len(must) + len(cannot)\n",
        "    return v / max(total, 1)\n"
      ],
      "metadata": {
        "id": "UvIB7IxrE2d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def fill_unassigned_by_nearest_center(X, labels, k, seed=42):\n",
        "    \"\"\"\n",
        "    Assigns any label == -1 to the nearest cluster centroid computed\n",
        "    from currently-assigned points. This is used ONLY to avoid metric\n",
        "    distortion from dropping edges incident to -1 nodes.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    labels_filled = labels.copy()\n",
        "\n",
        "    # compute centroids from assigned points\n",
        "    centers = np.zeros((k, X.shape[1]), dtype=np.float32)\n",
        "    for c in range(k):\n",
        "        idx = np.where(labels_filled == c)[0]\n",
        "        if len(idx) > 0:\n",
        "            centers[c] = X[idx].mean(axis=0)\n",
        "        else:\n",
        "            centers[c] = X[rng.integers(0, X.shape[0])]\n",
        "\n",
        "    # assign unassigned\n",
        "    un = np.where(labels_filled == -1)[0]\n",
        "    if len(un) > 0:\n",
        "        dists = np.linalg.norm(X[un, None, :] - centers[None, :, :], axis=2)\n",
        "        labels_filled[un] = np.argmin(dists, axis=1)\n",
        "\n",
        "    return labels_filled\n"
      ],
      "metadata": {
        "id": "vLEyh5roHVlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "def split_func_edges(edges, global_to_local_func, test_ratio=0.2, seed=42):\n",
        "    func_edges = []\n",
        "    for e in edges:\n",
        "        src, dst, et, w = safe_edge_fields(e)\n",
        "        if src in global_to_local_func and dst in global_to_local_func:\n",
        "            func_edges.append(e)\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = rng.permutation(len(func_edges))\n",
        "    cut = int(len(func_edges) * (1 - test_ratio))\n",
        "    train_edges = [func_edges[i] for i in idx[:cut]]\n",
        "    test_edges  = [func_edges[i] for i in idx[cut:]]\n",
        "    return train_edges, test_edges, len(func_edges)\n",
        "\n",
        "\n",
        "def cohesion_coupling_from_edges(labels, edges, global_to_local_func):\n",
        "    intra = 0\n",
        "    inter = 0\n",
        "    used = 0\n",
        "\n",
        "    for e in edges:\n",
        "        src, dst, et, w = safe_edge_fields(e)\n",
        "        if src in global_to_local_func and dst in global_to_local_func:\n",
        "            i = global_to_local_func[src]\n",
        "            j = global_to_local_func[dst]\n",
        "            ci = labels[i]\n",
        "            cj = labels[j]\n",
        "            # ignore unassigned\n",
        "            if ci == -1 or cj == -1:\n",
        "                continue\n",
        "            used += 1\n",
        "            if ci == cj:\n",
        "                intra += 1\n",
        "            else:\n",
        "                inter += 1\n",
        "\n",
        "    cohesion = intra / max(used, 1)\n",
        "    coupling = inter / max(used, 1)\n",
        "    return cohesion, coupling, used, intra, inter\n",
        "\n",
        "train_edges, test_edges, total_func_edges = split_func_edges(edges, global_to_local_func, test_ratio=0.2, seed=42)\n",
        "print(\"total_func_edges:\", total_func_edges, \"train:\", len(train_edges), \"test:\", len(test_edges))\n",
        "\n",
        "# Build constraints ONLY from train_edges\n",
        "Xg_norm = l2_normalize(X_gnn)\n",
        "must, cannot = build_constraints(\n",
        "    function_nodes=function_nodes,\n",
        "    edges=train_edges,\n",
        "    global_to_local_func=global_to_local_func,\n",
        "    X_norm=Xg_norm,\n",
        "    tau_must=0.90,\n",
        "    tau_cannot=0.00,\n",
        "    max_cannot=2000,\n",
        "    use_same_file_must=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# k=15\n",
        "K=10\n",
        "\n",
        "\n",
        "labels_gnn = cop_kmeans(X_gnn, k, must, cannot, max_iter=100, seed=42)\n",
        "\n",
        "# Fill unassigned so evaluation coverage is comparable\n",
        "labels_gnn = fill_unassigned_by_nearest_center(X_gnn, labels_gnn, k)\n",
        "\n",
        "# Evaluate ONLY on held-out test_edges\n",
        "coh_gnn, cou_gnn, used_gnn, intra_gnn, inter_gnn = cohesion_coupling_from_edges(labels_gnn, test_edges, global_to_local_func)\n",
        "print(f\"GNN (HOLDOUT) -> used_edges={used_gnn}, intra={intra_gnn}, inter={inter_gnn}, cohesion={coh_gnn:.4f}, coupling={cou_gnn:.4f}\")\n",
        "\n",
        "\n",
        "# --- LLM Clustering ---\n",
        "# Load LLM embeddings (assuming they are in node_embeddings.json)\n",
        "LLM_EMB_PATH = \"/content/node_embeddings.json\"\n",
        "with open(LLM_EMB_PATH, \"r\") as f:\n",
        "    llm_embeddings = json.load(f)\n",
        "\n",
        "X_llm = build_X_from_embeddings(llm_embeddings, func_global_ids)\n",
        "Xl_norm = l2_normalize(X_llm)\n",
        "\n",
        "# Build constraints for LLM, potentially with different thresholds\n",
        "must_llm, cannot_llm = build_constraints(\n",
        "    function_nodes=function_nodes,\n",
        "    edges=train_edges,\n",
        "    global_to_local_func=global_to_local_func,\n",
        "    X_norm=Xl_norm,\n",
        "    tau_must=0.90,\n",
        "    tau_cannot=0.00,\n",
        "    max_cannot=2000,\n",
        "    use_same_file_must=False\n",
        ")\n",
        "\n",
        "labels_llm = cop_kmeans(X_llm, k, must_llm, cannot_llm, max_iter=100, seed=42)\n",
        "labels_llm = fill_unassigned_by_nearest_center(X_llm, labels_llm, k)\n",
        "\n",
        "\n",
        "if HAS_LLM:\n",
        "    coh_llm, cou_llm, used_llm, intra_llm, inter_llm = cohesion_coupling_from_edges(labels_llm, test_edges, global_to_local_func)\n",
        "    print(f\"LLM (HOLDOUT) -> used_edges={used_llm}, intra={intra_llm}, inter={inter_llm}, cohesion={coh_llm:.4f}, coupling={cou_llm:.4f}\")\n",
        "\n",
        "\n",
        "coh_gnn, cou_gnn, used_gnn, intra_gnn, inter_gnn = cohesion_coupling_from_edges(labels_gnn, edges, global_to_local_func)\n",
        "print(f\"GNN clustering -> used_edges={used_gnn}, intra={intra_gnn}, inter={inter_gnn}, cohesion={coh_gnn:.4f}, coupling={cou_gnn:.4f}\")\n",
        "\n",
        "if HAS_LLM:\n",
        "    coh_llm, cou_llm, used_llm, intra_llm, inter_llm = cohesion_coupling_from_edges(labels_llm, edges, global_to_local_func)\n",
        "    print(f\"LLM clustering -> used_edges={used_llm}, intra={intra_llm}, inter={inter_llm}, cohesion={coh_llm:.4f}, coupling={cou_llm:.4f}\")"
      ],
      "metadata": {
        "id": "nVk9hFLjYsRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clusters_as_dict(labels, function_nodes):\n",
        "    clusters = defaultdict(list)\n",
        "    for idx, c in enumerate(labels):\n",
        "        if c == -1:\n",
        "            continue\n",
        "        clusters[int(c)].append(function_nodes[idx])\n",
        "    return dict(clusters)\n",
        "\n",
        "clusters_gnn = clusters_as_dict(labels_gnn, function_nodes)\n",
        "print(\"num_clusters_nonempty:\", len(clusters_gnn))\n",
        "print(\"sizes:\", sorted([len(v) for v in clusters_gnn.values()], reverse=True)[:8])"
      ],
      "metadata": {
        "id": "lEZspjvtE9qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time To Get OutPut"
      ],
      "metadata": {
        "id": "tEjfJT0fHxTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_cluster_groups(labels, function_nodes):\n",
        "    groups = defaultdict(list)\n",
        "    for i, c in enumerate(labels):\n",
        "        groups[int(c)].append(function_nodes[i])\n",
        "    return dict(groups)\n",
        "\n",
        "clusters_gnn = build_cluster_groups(labels_gnn, function_nodes)\n",
        "\n",
        "print(\"Non-empty clusters:\", len(clusters_gnn))\n",
        "print(\"Cluster sizes:\", sorted([len(v) for v in clusters_gnn.values()], reverse=True))\n"
      ],
      "metadata": {
        "id": "4P1NC9LwH07g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_name(n):\n",
        "    # adjust if your node schema uses different fields\n",
        "    return n.get(\"name\") or n.get(\"qualified_name\") or n.get(\"signature\") or str(n.get(\"id\"))\n",
        "\n",
        "for cid in sorted(clusters_gnn.keys()):\n",
        "    items = clusters_gnn[cid]\n",
        "    print(f\"\\n=== Microservice {cid} | size={len(items)} ===\")\n",
        "    for n in items[:15]:\n",
        "        print(\" -\", node_name(n))\n"
      ],
      "metadata": {
        "id": "isrWu1lGH4RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "func_to_cluster = []\n",
        "for i, fn in enumerate(function_nodes):\n",
        "    func_to_cluster.append({\n",
        "        \"id\": fn.get(\"id\"),\n",
        "        \"name\": node_name(fn),\n",
        "        \"file\": fn.get(\"file\"),\n",
        "        \"cluster\": int(labels_gnn[i]),\n",
        "    })\n",
        "\n",
        "with open(\"/content/gnn_clusters_function_map.json\", \"w\") as f:\n",
        "    json.dump(func_to_cluster, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", \"/content/gnn_clusters_function_map.json\")\n"
      ],
      "metadata": {
        "id": "_UPNo9e3H9Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_to_funcs = {}\n",
        "for cid, items in clusters_gnn.items():\n",
        "    cluster_to_funcs[str(cid)] = [{\n",
        "        \"id\": it.get(\"id\"),\n",
        "        \"name\": node_name(it),\n",
        "        \"file\": it.get(\"file\"),\n",
        "    } for it in items]\n",
        "\n",
        "with open(\"/content/gnn_clusters_grouped.json\", \"w\") as f:\n",
        "    json.dump(cluster_to_funcs, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", \"/content/gnn_clusters_grouped.json\")\n"
      ],
      "metadata": {
        "id": "JPp6MazuIFMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open(\"/content/gnn_clusters.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow([\"id\", \"name\", \"file\", \"cluster\"])\n",
        "    for row in func_to_cluster:\n",
        "        w.writerow([row[\"id\"], row[\"name\"], row[\"file\"], row[\"cluster\"]])\n",
        "\n",
        "print(\"Saved:\", \"/content/gnn_clusters.csv\")\n"
      ],
      "metadata": {
        "id": "6nBKWkUVIHNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding K\n"
      ],
      "metadata": {
        "id": "eqP3irUaIhs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = range(4, 30)\n",
        "\n",
        "results = []\n",
        "\n",
        "for k in range(8, 30):\n",
        "    # --- clustering ---\n",
        "    labels = cop_kmeans(X_gnn, k, must, cannot, max_iter=100, seed=42)\n",
        "    labels = fill_unassigned_by_nearest_center(X_gnn, labels, k)\n",
        "\n",
        "    # --- evaluation on HELD-OUT edges ---\n",
        "    coh, cou, used, intra, inter = cohesion_coupling_from_edges(\n",
        "        labels, test_edges, global_to_local_func\n",
        "    )\n",
        "\n",
        "    score = coh - cou   # λ = 1\n",
        "\n",
        "    cluster_sizes = [np.sum(labels == i) for i in range(k)]\n",
        "\n",
        "    results.append({\n",
        "        \"k\": k,\n",
        "        \"cohesion\": coh,\n",
        "        \"coupling\": cou,\n",
        "        \"score\": score,\n",
        "        \"min_size\": min(cluster_sizes),\n",
        "        \"max_size\": max(cluster_sizes),\n",
        "    })\n",
        "\n"
      ],
      "metadata": {
        "id": "SV3vXnOtIjS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_score = max(r[\"score\"] for r in results)\n",
        "\n",
        "candidates = [\n",
        "    r for r in results\n",
        "    if r[\"score\"] >= 0.95 * best_score\n",
        "    and r[\"min_size\"] >= 5\n",
        "    and r[\"max_size\"] <= 0.5 * len(function_nodes)\n",
        "]\n",
        "\n",
        "best_k = min(r[\"k\"] for r in candidates)\n",
        "print(\"Best k:\", best_k)\n"
      ],
      "metadata": {
        "id": "d7L7KEeDIouN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# --- Edge helpers (use your existing safe_edge_fields/global_to_local_func) ---\n",
        "def cohesion_coupling_from_edges(labels, edges, global_to_local_func):\n",
        "    intra = inter = used = 0\n",
        "    dropped_unassigned = 0\n",
        "    total_func_edges = 0\n",
        "\n",
        "    for e in edges:\n",
        "        src, dst, et, w = safe_edge_fields(e)\n",
        "        if src in global_to_local_func and dst in global_to_local_func:\n",
        "            total_func_edges += 1\n",
        "            i = global_to_local_func[src]\n",
        "            j = global_to_local_func[dst]\n",
        "            ci, cj = labels[i], labels[j]\n",
        "\n",
        "            if ci == -1 or cj == -1:\n",
        "                dropped_unassigned += 1\n",
        "                continue\n",
        "\n",
        "            used += 1\n",
        "            if ci == cj:\n",
        "                intra += 1\n",
        "            else:\n",
        "                inter += 1\n",
        "\n",
        "    cohesion = intra / max(used, 1)\n",
        "    coupling = inter / max(used, 1)\n",
        "    coverage = used / max(total_func_edges, 1)\n",
        "    return {\n",
        "        \"total_func_edges\": total_func_edges,\n",
        "        \"used_edges\": used,\n",
        "        \"dropped_unassigned\": dropped_unassigned,\n",
        "        \"coverage\": coverage,\n",
        "        \"intra\": intra,\n",
        "        \"inter\": inter,\n",
        "        \"cohesion\": cohesion,\n",
        "        \"coupling\": coupling,\n",
        "    }\n",
        "\n",
        "def silhouette_safe(X, labels):\n",
        "    labels = np.asarray(labels)\n",
        "    mask = labels != -1\n",
        "    X2 = X[mask]\n",
        "    y2 = labels[mask]\n",
        "\n",
        "    # silhouette needs at least 2 clusters and no empty/degenerate cases\n",
        "    uniq = np.unique(y2)\n",
        "    if len(uniq) < 2 or len(y2) < 3:\n",
        "        return None\n",
        "    # also fails if any cluster has size 1 (can be allowed but may be unstable)\n",
        "    counts = {u: int(np.sum(y2 == u)) for u in uniq}\n",
        "    if min(counts.values()) < 2:\n",
        "        # still can compute, but often noisy; return anyway if you want:\n",
        "        try:\n",
        "            return float(silhouette_score(X2, y2, metric=\"euclidean\"))\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    try:\n",
        "        return float(silhouette_score(X2, y2, metric=\"euclidean\"))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# -----------------------\n",
        "# RUN METRICS (FULL EDGES)\n",
        "# -----------------------\n",
        "metrics_full = cohesion_coupling_from_edges(labels_gnn, edges, global_to_local_func)\n",
        "sil_full = silhouette_safe(X_gnn, labels_gnn)\n",
        "\n",
        "print(\"=== GNN clustering (FULL) ===\")\n",
        "print(f\"coverage={metrics_full['coverage']:.3f}  used_edges={metrics_full['used_edges']}/{metrics_full['total_func_edges']}  dropped_unassigned={metrics_full['dropped_unassigned']}\")\n",
        "print(f\"intra={metrics_full['intra']}  inter={metrics_full['inter']}\")\n",
        "print(f\"cohesion={metrics_full['cohesion']:.4f}  coupling={metrics_full['coupling']:.4f}\")\n",
        "print(\"silhouette:\", \"N/A\" if sil_full is None else f\"{sil_full:.4f}\")\n",
        "\n",
        "# --------------------------\n",
        "# OPTIONAL: HOLDOUT METRICS\n",
        "# --------------------------\n",
        "# If you have test_edges from your split, you can evaluate on holdout too:\n",
        "if \"test_edges\" in globals():\n",
        "    metrics_test = cohesion_coupling_from_edges(labels_gnn, test_edges, global_to_local_func)\n",
        "    sil_test = silhouette_safe(X_gnn, labels_gnn)  # silhouette is geometry-only; no \"holdout\"\n",
        "\n",
        "    print(\"\\n=== GNN clustering (HOLDOUT edges) ===\")\n",
        "    print(f\"coverage={metrics_test['coverage']:.3f}  used_edges={metrics_test['used_edges']}/{metrics_test['total_func_edges']}  dropped_unassigned={metrics_test['dropped_unassigned']}\")\n",
        "    print(f\"intra={metrics_test['intra']}  inter={metrics_test['inter']}\")\n",
        "    print(f\"cohesion={metrics_test['cohesion']:.4f}  coupling={metrics_test['coupling']:.4f}\")\n",
        "    print(\"silhouette (same as full):\", \"N/A\" if sil_test is None else f\"{sil_test:.4f}\")\n",
        "\n",
        "# --------------------------\n",
        "# OPTIONAL: LLM COMPARISON\n",
        "# --------------------------\n",
        "if \"labels_llm\" in globals() and \"X_llm\" in globals():\n",
        "    m_llm = cohesion_coupling_from_edges(labels_llm, edges, global_to_local_func)\n",
        "    sil_llm = silhouette_safe(X_llm, labels_llm)\n",
        "\n",
        "    print(\"\\n=== LLM clustering (FULL) ===\")\n",
        "    print(f\"coverage={m_llm['coverage']:.3f}  used_edges={m_llm['used_edges']}/{m_llm['total_func_edges']}  dropped_unassigned={m_llm['dropped_unassigned']}\")\n",
        "    print(f\"intra={m_llm['intra']}  inter={m_llm['inter']}\")\n",
        "    print(f\"cohesion={m_llm['cohesion']:.4f}  coupling={m_llm['coupling']:.4f}\")\n",
        "    print(\"silhouette:\", \"N/A\" if sil_llm is None else f\"{sil_llm:.4f}\")\n",
        "    #K=10\n"
      ],
      "metadata": {
        "id": "o52oT__aQcVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZnugUNVuIzPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to **GNN (FULL)** vs **LLM (FULL)**:\n",
        "\n",
        "1. **Cohesion** increased from `0.4721` to `0.6034`\n",
        "   → **+0.1313** (about **+13.13 percentage points**).\n",
        "   In simple terms: a larger share of edges ended up **within** clusters.\n",
        "\n",
        "2. **Coupling** decreased from `0.5279` to `0.3966`\n",
        "   → **−0.1313** (about **−13.13 percentage points**, which is better).\n",
        "   Meaning: fewer edges were **between** clusters.\n",
        "\n",
        "3. **Silhouette** increased from `0.0510` to `0.0951`\n",
        "   → **+0.0441** (about **1.86×** higher).\n"
      ],
      "metadata": {
        "id": "-au1A-y5Q__Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvis.network import Network\n",
        "import random\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Color palette (stable, distinct)\n",
        "# ----------------------------\n",
        "PALETTE = [\n",
        "    \"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\",\n",
        "    \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\",\n",
        "    \"#393b79\", \"#637939\", \"#8c6d31\", \"#843c39\", \"#7b4173\"\n",
        "]\n",
        "\n",
        "def color_for_cluster(c):\n",
        "    if c is None or c == -1:\n",
        "        return \"#c7c7c7\"  # unassigned/unknown\n",
        "    return PALETTE[int(c) % len(PALETTE)]\n",
        "\n",
        "def node_display_name(n):\n",
        "    return n.get(\"name\") or n.get(\"qualified_name\") or n.get(\"signature\") or str(n.get(\"id\"))\n",
        "\n",
        "def safe_edge_fields(e):\n",
        "    src = e.get(\"src\", e.get(\"source\", e.get(\"from\")))\n",
        "    dst = e.get(\"dst\", e.get(\"target\", e.get(\"to\")))\n",
        "    et  = e.get(\"type\", e.get(\"label\", \"\"))\n",
        "    w   = e.get(\"weight\", e.get(\"count\", 1))\n",
        "    return src, dst, et, w\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Build node_id -> cluster map (for function nodes)\n",
        "# ----------------------------\n",
        "func_id_to_cluster = {}\n",
        "for i, fn in enumerate(function_nodes):\n",
        "    func_id_to_cluster[fn[\"id\"]] = int(labels_gnn[i])\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Create network\n",
        "# ----------------------------\n",
        "net = Network(height=\"800px\", width=\"100%\", bgcolor=\"#ffffff\", font_color=\"#111111\", directed=True)\n",
        "net.barnes_hut(gravity=-8000, central_gravity=0.3, spring_length=120, spring_strength=0.01, damping=0.09)\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Add nodes with cluster colors\n",
        "# ----------------------------\n",
        "for n in nodes:\n",
        "    nid = n.get(\"id\")\n",
        "    ntype = str(n.get(\"type\", \"\"))\n",
        "    c = func_id_to_cluster.get(nid, None) if ntype.lower() == \"function\" else None\n",
        "\n",
        "    title_parts = [\n",
        "        f\"Type: {ntype}\",\n",
        "        f\"ID: {nid}\",\n",
        "    ]\n",
        "    if c is not None:\n",
        "        title_parts.append(f\"Cluster: {c}\")\n",
        "\n",
        "    # size: make function nodes slightly larger\n",
        "    size = 18 if ntype.lower() == \"function\" else 10\n",
        "\n",
        "    net.add_node(\n",
        "        nid,\n",
        "        label=node_display_name(n),\n",
        "        title=\"<br>\".join(title_parts),\n",
        "        color=color_for_cluster(c) if ntype.lower() == \"function\" else \"#e0e0e0\",\n",
        "        size=size,\n",
        "    )\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Add edges\n",
        "# ----------------------------\n",
        "for e in edges:\n",
        "    src, dst, et, w = safe_edge_fields(e)\n",
        "    if src is None or dst is None:\n",
        "        continue\n",
        "\n",
        "    # optional: edge width by weight (kept bounded)\n",
        "    try:\n",
        "        width = max(1, min(6, float(w)))\n",
        "    except Exception:\n",
        "        width = 1\n",
        "\n",
        "    net.add_edge(\n",
        "        src,\n",
        "        dst,\n",
        "        title=str(et),\n",
        "        width=width\n",
        "    )\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Legend (clusters) as dummy nodes (optional but useful)\n",
        "# ----------------------------\n",
        "# Add a small legend column on the left\n",
        "legend_x = -800\n",
        "legend_y = -400\n",
        "for c in sorted(set(func_id_to_cluster.values())):\n",
        "    net.add_node(\n",
        "        f\"legend_cluster_{c}\",\n",
        "        label=f\"Cluster {c}\",\n",
        "        color=color_for_cluster(c),\n",
        "        shape=\"box\",\n",
        "        x=legend_x,\n",
        "        y=legend_y + 60 * c,\n",
        "        fixed=True,\n",
        "        physics=False,\n",
        "        size=14,\n",
        "    )\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Save HTML\n",
        "# ----------------------------\n",
        "out_path = \"/content/graph_clustered.html\"\n",
        "\n",
        "# IMPORTANT: write_html avoids some notebook/template issues that show() triggers\n",
        "net.write_html(out_path, open_browser=False)\n",
        "print(\"Saved clustered graph to:\", out_path)\n"
      ],
      "metadata": {
        "id": "ix66USR9Rflb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "def find_optimal_k(embeddings, max_k=20):\n",
        "    scores = []\n",
        "    k_values = range(2, max_k + 1)\n",
        "\n",
        "    for k in k_values:\n",
        "        # شبیه‌سازی کلاستری که در مدل GNN استفاده کردید\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        labels = kmeans.fit_predict(embeddings)\n",
        "        score = silhouette_score(embeddings, labels)\n",
        "        scores.append(score)\n",
        "        print(f\"K={k}, Silhouette Score={score:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_values, scores, 'bo-')\n",
        "    plt.xlabel('Number of Clusters (K)')\n",
        "    plt.ylabel('Silhouette Score')\n",
        "    plt.title('Optimal K Selection (Elbow Method)')\n",
        "    plt.axvline(x=10, color='r', linestyle='--', label='Selected K')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# اجرا روی ویژگی‌های استخراج شده\n",
        "find_optimal_k(X_gnn)\n"
      ],
      "metadata": {
        "id": "W_W6Sk8ofCYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}